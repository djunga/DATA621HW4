---
title: "HW 4"
author: Deepika Dilip, Tora Mullings, Daniel Sullivan, Deepa Sharma, Bikram Barua,
  Newman Okereafor
date: '2022-11-10'
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning = F, message = F)
```


```{r, echo=FALSE}
library(tidyverse)
library(ggplot2)
library(ggthemes)
library(corrplot)
library(reshape2)
library(knitr)
library(broom)
library(caret)
library(leaps)
library(MASS)
library(magrittr)
library(betareg)
library(pscl)
library(gtsummary)
library(nnet)
```

```{r}
insurance.train <- read.csv('https://raw.githubusercontent.com/djunga/DATA621HW4/main/insurance_train.csv')
insurance.eval <- read.csv('https://raw.githubusercontent.com/djunga/DATA621HW4/main/insurance_eval.csv')

insurance.all = rbind(data.frame(insurance.train, dataset = "train"), data.frame(insurance.eval, dataset = "eval"))
```


# Data Preparation and Exploratory Data Analysis

Prior to data analysis, we'll need to clean the dataset and convert variables to accurate classes. We can convert home valuation, bluebook values, and income to numeric variables while designating other variables as categorical:

```{r}
insurance.all = sapply(insurance.all, function(x) x = gsub("z\\_|\\<|<", "", x)) %>% as.data.frame()
insurance.all = sapply(insurance.all, function(x) x = ifelse(x == "", NA, x)) %>% as.data.frame()

# Money to numeric
insurance.all$HOME_VAL = gsub("(\\$|\\,)", "", insurance.all$HOME_VA) %>% as.numeric()
insurance.all$BLUEBOOK = gsub("(\\$|\\,)", "", insurance.all$BLUEBOOK) %>% as.numeric()
insurance.all$INCOME = gsub("(\\$|\\,)", "", insurance.all$INCOME) %>% as.numeric()
insurance.all$OLDCLAIM = gsub("(\\$|\\,)", "", insurance.all$OLDCLAIM) %>% as.numeric()

# Character to numeric
insurance.all = insurance.all %>% mutate(CAR_AGE = as.numeric(CAR_AGE), AGE = as.numeric(AGE), HOMEKIDS = as.numeric(HOMEKIDS), YOJ = as.numeric(YOJ),TRAVTIME = as.numeric(TRAVTIME),  TIF = as.numeric(TIF), CLM_FREQ = as.numeric(CLM_FREQ), MVR_PTS = as.numeric(MVR_PTS),CAR_AGE = as.numeric(CAR_AGE), TARGET_AMT = as.numeric(TARGET_AMT), KIDSDRIV = as.numeric(KIDSDRIV) )


insurance.all = insurance.all %>% mutate(INDEX = as.integer(INDEX), TARGET_FLAG = as.factor(TARGET_FLAG), PARENT1 = as.factor(PARENT1), MSTATUS = as.factor(MSTATUS), SEX = as.factor(SEX), EDUCATION = as.factor(EDUCATION), JOB = as.factor(JOB),CAR_USE = as.factor(CAR_USE), CAR_TYPE = as.factor(CAR_TYPE),  RED_CAR = as.factor(RED_CAR), REVOKED = as.factor(REVOKED), URBANICITY = as.factor(URBANICITY))

insurance.train = insurance.all %>% filter(dataset == "train") %>% select(-"dataset")
```


First, we'll create a correlation plot to visualize the associations between variables:

```{r}
corrplot(cor( select_if(insurance.train, is.numeric), use = "complete.obs"), tl.col="black", tl.cex=0.6, order='AOE')
```

## Visualizing Linearity:

```{r}

mlt.train = insurance.train  %>% select_if(is.numeric) 
mlt.train = melt(mlt.train, id.vars = c("INDEX", "TARGET_AMT"))

ggplot(aes(value, TARGET_AMT), data = mlt.train) + geom_point() + facet_wrap(~variable, scales = "free") + labs(title = "Distributions of Continuous Variables", x = "Variable", y = "TARGET_AMT") 
```

## Histograms:

We can make some histograms to analyze distributions:

```{r}

mlt.train = insurance.train  %>% select_if(is.numeric) 
mlt.train = melt(mlt.train, id.vars = "INDEX")

ggplot(aes(value), data = mlt.train) + geom_histogram(stat = "bin", fill = "navyblue") + facet_wrap(~variable, scales = "free") + labs(title = "Distributions of Continuous Variables", x = "Variable", y = "Count") 
```



To analyze data distribution, we can make a couple of bar plots:

```{r}

mlt.train = insurance.train  %>% select_if(is.factor) 
mlt.train = cbind(mlt.train, select(insurance.train, INDEX))
mlt.train = melt(mlt.train, id.vars = "INDEX")
mlt.train = mlt.train %>%  group_by(variable, value) %>% summarise(n = n())

ggplot(aes(value, n), data = mlt.train) + geom_bar(stat = "identity", fill = "navyblue") + facet_wrap(~variable, scales = "free") + labs(title = "Distributions of Categorical Variables", x = "Variable", y = "Count") 
```


And lastly, we can take a look at missing data:

```{r}

mlt.train = insurance.train 
mlt.train = melt(mlt.train, id.vars = "INDEX") %>% filter(is.na(value)) %>% group_by(variable) %>% summarise(n = n())

ggplot(aes(reorder(variable, -n), n), data =mlt.train ) + geom_bar(stat = "identity") + labs(x = "Variable", y = "# of Missing Values", title = "Count of Missing Values")
```




## Key Interpretations:

1. Having kids at home has a strong negative correlation with age. This is expected
2. Most folks in this dataset live in urban settings, are female, married, and use cars for commercial use. 


# Modeling `TARGET_AMT` per Multiple Linear Regression

## Model 1: Full Model

We can start by creating a model where we utilize all predictors in the dataset. Ideally, this will give us a sense of what the leading contributors are towards the model:

```{r}
GLM.model1 = lm(TARGET_AMT ~., data = select(insurance.train, -c("INDEX", "TARGET_FLAG")))
tbl_regression(GLM.model1) %>% add_glance_table()

```

The R2 here isn't the best fit--only 7% of the variance is explained. We can use a simple method and select features of interest based on statistical significance; we can use a p-threshold of less than 0.1 to start:

## Model 2: LM with selected features
```{r}

GLM.model2 = lm(TARGET_AMT ~ SEX + JOB + BLUEBOOK + EDUCATION + CAR_TYPE +  CLM_FREQ + REVOKED + MVR_PTS + CAR_AGE ,  select(insurance.train, -c("INDEX", "TARGET_FLAG")))
tbl_regression(GLM.model2) %>% add_glance_table()
```

The R2 takes a hit (0.07 to 0.04), but this might a better model: more observations are used due to no longer requiring complete cases, plus most of the information from the predictors support the model. 

## Model 3: Log Transformation

We know from previous plots that the target amount is heavily skewed right. Therefore, we can log-transform it and normalize the outcome:

```{r}

insurance.train = insurance.train %>% mutate(logTARGET_AMT = log(TARGET_AMT + 1e-06))
vec.cols.model3 = c("SEX", "BLUEBOOK", "EDUCATION", "CAR_TYPE", "REVOKED", "MVR_PTS", "CAR_AGE")
GLM.model3 = lm(logTARGET_AMT ~  SEX + BLUEBOOK + EDUCATION + CAR_TYPE + REVOKED + MVR_PTS + CAR_AGE , data = select(insurance.train, -"INDEX"))
tbl_regression(GLM.model3) %>% add_glance_table()
```

The R-squared here goes up to 0.1, but the intercept is significant, hinting that not all the information is explained. 


## Model 4: Full Model: Cross-Validation

One last approach we can take is using cross-validation to create multiple folds and partition the dataset while training:

```{r}
library(caret)

insurance.train.complete = insurance.train %>% select(-c("TARGET_AMT", "TARGET_FLAG")) %>%  na.omit()

fit.control <- trainControl(method = "repeatedcv", number = 5, repeats = 10)

cv_fit <- train(logTARGET_AMT ~  SEX + BLUEBOOK + EDUCATION + CAR_TYPE + REVOKED + MVR_PTS + CAR_AGE, data = insurance.train.complete, method = "glm",  family = "gaussian", trControl = fit.control)

cv_fit$results %>% knitr::kable()
 
```

The R-Squared here is 0.1, but it might perform better due to cross-validation.




# Evaluating Linear Regression Models

```{r}
insurance.eval = insurance.all %>% filter(dataset == "eval") %>% select(-"dataset")

eval.model1.glm = predict(GLM.model1)
eval.model2.glm = predict(GLM.model2, select(insurance.eval, -"TARGET_AMT"))
eval.model3.glm = 10^predict(GLM.model3, select(insurance.eval, -"TARGET_AMT"))
eval.model4.glm = 10^predict(cv_fit, newdata = select(insurance.eval, -"TARGET_AMT"))

model_predictions = rbind(data.frame(name = "Model1", predict = eval.model1.glm),
data.frame(name = "Model2", predict = eval.model2.glm),
data.frame(name = "Model3", predict = eval.model3.glm),
data.frame(name = "CV", predict = eval.model4.glm))


ggplot(aes(x = predict), data = model_predictions) + geom_histogram() + facet_wrap(~name) + labs(x = "Income", y = "Count", title = "Predicted Values")
```



## Model 5: Linear Discriminant Analysis (LDA), Manual Variable Selection

We are building model that can predict the probability that a person will crash their car. The variables in this model are manually selected:
- AGE
- CLM_FREQ
- JOB
- REVOKED
- MVR_PTS
- MSTATUS
- CAR_TYPE

We theorize that these variables are positively related to the driver's personal accountability. In other words, a driver who positively associated with these variables is theorized to be more prone to crashing their car.

```{r}
train.df <- insurance.train %>% select(-c("TARGET_AMT", "INDEX", "logTARGET_AMT"))
mlda <- lda(TARGET_FLAG ~ AGE + CLM_FREQ +JOB+ REVOKED+MVR_PTS + MSTATUS + CAR_TYPE , data=train.df)

lda_predict <- predict(mlda, newdata=select(train.df, -"TARGET_FLAG"))$class
table(lda_predict, train.df[,1])
```

### Accuracy:
```{r}
mean(lda_predict==train.df[,1], na.rm=TRUE)
```
```{r}
conf <- table(list(predicted=lda_predict, observed=train.df[,1]))
confusionMatrix(conf)
```



# Appendix -----


```{r, eval = F, echo = T}
## ----setup, include=FALSE---------------------------------------------------------------------------------------
knitr::opts_chunk$set(echo = F, warning = F, message = F)


## ---- echo=FALSE------------------------------------------------------------------------------------------------
library(tidyverse)
library(ggplot2)
library(ggthemes)
library(corrplot)
library(reshape2)
library(knitr)
library(broom)
library(caret)
library(leaps)
library(MASS)
library(magrittr)
library(betareg)
library(pscl)
library(gtsummary)
library(nnet)


## ---------------------------------------------------------------------------------------------------------------
insurance.train <- read.csv('https://raw.githubusercontent.com/djunga/DATA621HW4/main/insurance_train.csv')
insurance.eval <- read.csv('https://raw.githubusercontent.com/djunga/DATA621HW4/main/insurance_eval.csv')

insurance.all = rbind(data.frame(insurance.train, dataset = "train"), data.frame(insurance.eval, dataset = "eval"))


## ---------------------------------------------------------------------------------------------------------------
insurance.all = sapply(insurance.all, function(x) x = gsub("z\\_|\\<|<", "", x)) %>% as.data.frame()
insurance.all = sapply(insurance.all, function(x) x = ifelse(x == "", NA, x)) %>% as.data.frame()

# Money to numeric
insurance.all$HOME_VAL = gsub("(\\$|\\,)", "", insurance.all$HOME_VA) %>% as.numeric()
insurance.all$BLUEBOOK = gsub("(\\$|\\,)", "", insurance.all$BLUEBOOK) %>% as.numeric()
insurance.all$INCOME = gsub("(\\$|\\,)", "", insurance.all$INCOME) %>% as.numeric()
insurance.all$OLDCLAIM = gsub("(\\$|\\,)", "", insurance.all$OLDCLAIM) %>% as.numeric()

# Character to numeric
insurance.all = insurance.all %>% mutate(CAR_AGE = as.numeric(CAR_AGE), AGE = as.numeric(AGE), HOMEKIDS = as.numeric(HOMEKIDS), YOJ = as.numeric(YOJ),TRAVTIME = as.numeric(TRAVTIME),  TIF = as.numeric(TIF), CLM_FREQ = as.numeric(CLM_FREQ), MVR_PTS = as.numeric(MVR_PTS),CAR_AGE = as.numeric(CAR_AGE), TARGET_AMT = as.numeric(TARGET_AMT), KIDSDRIV = as.numeric(KIDSDRIV) )


insurance.all = insurance.all %>% mutate(INDEX = as.integer(INDEX), TARGET_FLAG = as.factor(TARGET_FLAG), PARENT1 = as.factor(PARENT1), MSTATUS = as.factor(MSTATUS), SEX = as.factor(SEX), EDUCATION = as.factor(EDUCATION), JOB = as.factor(JOB),CAR_USE = as.factor(CAR_USE), CAR_TYPE = as.factor(CAR_TYPE),  RED_CAR = as.factor(RED_CAR), REVOKED = as.factor(REVOKED), URBANICITY = as.factor(URBANICITY))

insurance.train = insurance.all %>% filter(dataset == "train") %>% select(-"dataset")


## ---------------------------------------------------------------------------------------------------------------
corrplot(cor( select_if(insurance.train, is.numeric), use = "complete.obs"), tl.col="black", tl.cex=0.6, order='AOE')


## ---------------------------------------------------------------------------------------------------------------

mlt.train = insurance.train  %>% select_if(is.numeric) 
mlt.train = melt(mlt.train, id.vars = c("INDEX", "TARGET_AMT"))

ggplot(aes(value, TARGET_AMT), data = mlt.train) + geom_point() + facet_wrap(~variable, scales = "free") + labs(title = "Distributions of Continuous Variables", x = "Variable", y = "TARGET_AMT") 


## ---------------------------------------------------------------------------------------------------------------

mlt.train = insurance.train  %>% select_if(is.numeric) 
mlt.train = melt(mlt.train, id.vars = "INDEX")

ggplot(aes(value), data = mlt.train) + geom_histogram(stat = "bin", fill = "navyblue") + facet_wrap(~variable, scales = "free") + labs(title = "Distributions of Continuous Variables", x = "Variable", y = "Count") 


## ---------------------------------------------------------------------------------------------------------------

mlt.train = insurance.train  %>% select_if(is.factor) 
mlt.train = cbind(mlt.train, select(insurance.train, INDEX))
mlt.train = melt(mlt.train, id.vars = "INDEX")
mlt.train = mlt.train %>%  group_by(variable, value) %>% summarise(n = n())

ggplot(aes(value, n), data = mlt.train) + geom_bar(stat = "identity", fill = "navyblue") + facet_wrap(~variable, scales = "free") + labs(title = "Distributions of Categorical Variables", x = "Variable", y = "Count") 


## ---------------------------------------------------------------------------------------------------------------

mlt.train = insurance.train 
mlt.train = melt(mlt.train, id.vars = "INDEX") %>% filter(is.na(value)) %>% group_by(variable) %>% summarise(n = n())

ggplot(aes(reorder(variable, -n), n), data =mlt.train ) + geom_bar(stat = "identity") + labs(x = "Variable", y = "# of Missing Values", title = "Count of Missing Values")


## ---------------------------------------------------------------------------------------------------------------
GLM.model1 = lm(TARGET_AMT ~., data = select(insurance.train, -c("INDEX", "TARGET_FLAG")))
tbl_regression(GLM.model1) %>% add_glance_table()



## ---------------------------------------------------------------------------------------------------------------

GLM.model2 = lm(TARGET_AMT ~ SEX + JOB + BLUEBOOK + EDUCATION + CAR_TYPE +  CLM_FREQ + REVOKED + MVR_PTS + CAR_AGE ,  select(insurance.train, -c("INDEX", "TARGET_FLAG")))
tbl_regression(GLM.model2) %>% add_glance_table()


## ---------------------------------------------------------------------------------------------------------------

insurance.train = insurance.train %>% mutate(logTARGET_AMT = log(TARGET_AMT + 1e-06))
vec.cols.model3 = c("SEX", "BLUEBOOK", "EDUCATION", "CAR_TYPE", "REVOKED", "MVR_PTS", "CAR_AGE")
GLM.model3 = lm(logTARGET_AMT ~  SEX + BLUEBOOK + EDUCATION + CAR_TYPE + REVOKED + MVR_PTS + CAR_AGE , data = select(insurance.train, -"INDEX"))
tbl_regression(GLM.model3) %>% add_glance_table()


## ---------------------------------------------------------------------------------------------------------------
library(caret)

insurance.train.complete = insurance.train %>% select(-c("TARGET_AMT", "TARGET_FLAG")) %>%  na.omit()

fit.control <- trainControl(method = "repeatedcv", number = 5, repeats = 10)

cv_fit <- train(logTARGET_AMT ~  SEX + BLUEBOOK + EDUCATION + CAR_TYPE + REVOKED + MVR_PTS + CAR_AGE, data = insurance.train.complete, method = "glm",  family = "gaussian", trControl = fit.control)

cv_fit$results %>% knitr::kable()
 


## ---------------------------------------------------------------------------------------------------------------
insurance.eval = insurance.all %>% filter(dataset == "eval") %>% select(-"dataset")

eval.model1.glm = predict(GLM.model1)
eval.model2.glm = predict(GLM.model2, select(insurance.eval, -"TARGET_AMT"))
eval.model3.glm = 10^predict(GLM.model3, select(insurance.eval, -"TARGET_AMT"))
eval.model4.glm = 10^predict(cv_fit, newdata = select(insurance.eval, -"TARGET_AMT"))

model_predictions = rbind(data.frame(name = "Model1", predict = eval.model1.glm),
data.frame(name = "Model2", predict = eval.model2.glm),
data.frame(name = "Model3", predict = eval.model3.glm),
data.frame(name = "CV", predict = eval.model4.glm))


ggplot(aes(x = predict), data = model_predictions) + geom_histogram() + facet_wrap(~name) + labs(x = "Income", y = "Count", title = "Predicted Values")
```